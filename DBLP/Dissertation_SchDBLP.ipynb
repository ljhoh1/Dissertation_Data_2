{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "g7j3rooghYjZ",
   "metadata": {
    "id": "g7j3rooghYjZ"
   },
   "source": [
    "# Loading Material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlrnCpFeh24V",
   "metadata": {
    "id": "jlrnCpFeh24V"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eHBRiTCfse1o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHBRiTCfse1o",
    "outputId": "e9fce0ea-685d-4dc6-dd7b-7d96e2476990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py_entitymatching\n",
      "  Downloading py_entitymatching-0.4.0.tar.gz (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (2.23.0)\n",
      "Collecting ipython>=5.6\n",
      "  Downloading ipython-7.26.0-py3-none-any.whl (786 kB)\n",
      "\u001b[K     |████████████████████████████████| 786 kB 51.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (3.2.2)\n",
      "Collecting PyPrind\n",
      "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting py-stringsimjoin>=0.3.0\n",
      "  Downloading py_stringsimjoin-0.3.2.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 50.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (2.4.7)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from py_entitymatching) (1.19.5)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)\n",
      "\u001b[K     |████████████████████████████████| 368 kB 58.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (0.18.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (4.4.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (5.0.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.6->py_entitymatching) (57.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=5.6->py_entitymatching) (0.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.4->py_entitymatching) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.4->py_entitymatching) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.4->py_entitymatching) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2.4->py_entitymatching) (1.15.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=5.6->py_entitymatching) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.6->py_entitymatching) (0.2.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from py-stringsimjoin>=0.3.0->py_entitymatching) (1.0.1)\n",
      "Requirement already satisfied: pandas>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from py-stringsimjoin>=0.3.0->py_entitymatching) (1.1.5)\n",
      "Collecting py_stringmatching>=0.2.1\n",
      "  Downloading py_stringmatching-0.4.2.tar.gz (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 58.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.16.0->py-stringsimjoin>=0.3.0->py_entitymatching) (2018.9)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=5.6->py_entitymatching) (0.2.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py_entitymatching) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py_entitymatching) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py_entitymatching) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py_entitymatching) (3.0.4)\n",
      "Building wheels for collected packages: py-entitymatching, py-stringsimjoin, py-stringmatching\n",
      "  Building wheel for py-entitymatching (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-entitymatching: filename=py_entitymatching-0.4.0-cp37-cp37m-linux_x86_64.whl size=2634168 sha256=cc7052b709e8726120cf569d50147cb76b727aebce4eb77ddb69b34dae122ff5\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/ad/74/76750f1ddbf65ec97e582e64d7e3b0848ad21658d6a8ffbe56\n",
      "  Building wheel for py-stringsimjoin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-stringsimjoin: filename=py_stringsimjoin-0.3.2-cp37-cp37m-linux_x86_64.whl size=3724228 sha256=68b5fae2c9d05d7b50318a9710069357e7e7bf29a01ce2d8dff4d78ec6149eae\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/0d/a4/c0a779a4d196bed8ed41ca12be09a93e158e4fc974b59f9fd5\n",
      "  Building wheel for py-stringmatching (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-stringmatching: filename=py_stringmatching-0.4.2-cp37-cp37m-linux_x86_64.whl size=2055527 sha256=e2c047a1fc6c8288e0493e234e0225340b3299291ab04bd9730177f6cc17a591\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/12/86/46f9378ae730550dd2f89cad50880e70d66cabedd90074f2d2\n",
      "Successfully built py-entitymatching py-stringsimjoin py-stringmatching\n",
      "Installing collected packages: PyPrind, py-stringmatching, prompt-toolkit, py-stringsimjoin, ipython, py-entitymatching\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 5.5.0\n",
      "    Uninstalling ipython-5.5.0:\n",
      "      Successfully uninstalled ipython-5.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.19 which is incompatible.\n",
      "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.26.0 which is incompatible.\u001b[0m\n",
      "Successfully installed PyPrind-2.11.3 ipython-7.26.0 prompt-toolkit-3.0.19 py-entitymatching-0.4.0 py-stringmatching-0.4.2 py-stringsimjoin-0.3.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "prompt_toolkit"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping preprocessing as it is not installed.\u001b[0m\n",
      "Found existing installation: fastai 1.0.61\n",
      "Uninstalling fastai-1.0.61:\n",
      "  Successfully uninstalled fastai-1.0.61\n",
      "\u001b[33mWARNING: Skipping allennlp as it is not installed.\u001b[0m\n",
      "Collecting deepmatcher\n",
      "  Downloading deepmatcher-0.1.2.post2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (1.9.0+cu102)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (4.41.1)\n",
      "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (2.11.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (1.15.0)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (0.29.23)\n",
      "Requirement already satisfied: torchtext>=0.9 in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (0.10.0)\n",
      "Requirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (3.2.5)\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 3.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (1.1.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (0.3.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepmatcher) (0.22.2.post1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->deepmatcher) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.9->deepmatcher) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.9->deepmatcher) (2.23.0)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext->deepmatcher) (57.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deepmatcher) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepmatcher) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.9->deepmatcher) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.9->deepmatcher) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.9->deepmatcher) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext>=0.9->deepmatcher) (2021.5.30)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepmatcher) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepmatcher) (1.4.1)\n",
      "Building wheels for collected packages: deepmatcher, fasttext\n",
      "  Building wheel for deepmatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deepmatcher: filename=deepmatcher-0.1.2.post2-py2.py3-none-any.whl size=54219 sha256=40ce8218e2b99cd48e685334e77b8da6df8db048db7acb9cfb1d45e7f8a4b46f\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/42/1d/81a0ff6bd1a807a9cc51c4a2fc5459292d5d486346ce925481\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3094811 sha256=5be8bfab23d57eb4810055b1c9802e29e6c9287cc0a874a68c63b0ab1b517465\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built deepmatcher fasttext\n",
      "Installing collected packages: pybind11, fasttext, deepmatcher\n",
      "Successfully installed deepmatcher-0.1.2.post2 fasttext-0.9.2 pybind11-2.7.1\n",
      "Collecting pandas==1.2.4\n",
      "  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 4.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.4) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.4 which is incompatible.\u001b[0m\n",
      "Successfully installed pandas-1.2.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pandas"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify pandas==1.1.5\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "try:\n",
    "    import py_entitymatching as em\n",
    "except:\n",
    "    !pip install py_entitymatching\n",
    "\n",
    "try:\n",
    "    import deepmatcher\n",
    "except:\n",
    "    !pip uninstall preprocessing -y\n",
    "    !pip uninstall fastai -y \n",
    "    !pip uninstall allennlp -y\n",
    "    !pip install deepmatcher\n",
    "\n",
    "import pandas as pd\n",
    "if pd.__version__ != \"1.2.4\":\n",
    " !pip install pandas==1.2.4\n",
    " import pandas as pd\n",
    " print(f\"Verify pandas=={pd.__version__}\")\n",
    "else:\n",
    " print(f\"Verify pandas=={pd.__version__}\")\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import deepmatcher as dm\n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6_4M1sgM_ZQb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_4M1sgM_ZQb",
    "outputId": "7d8ecb01-c641-4490-c1b4-04908dd8683b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Dissertation_Data_2'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 39 (delta 12), reused 32 (delta 7), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "# get datasets from directoy\n",
    "! git clone https://github.com/ljhoh1/Dissertation_Data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rHq8rVMQjf9Z",
   "metadata": {
    "id": "rHq8rVMQjf9Z"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vIRp-S0830ST",
   "metadata": {
    "id": "vIRp-S0830ST"
   },
   "outputs": [],
   "source": [
    "# The path to the two input tables.\n",
    "path_Scholar = os.path.join('.', 'Dissertation_Data_2', 'Scholar_data.csv')\n",
    "path_DBLP = os.path.join('.', 'Dissertation_Data_2', 'DBLP1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938f2e8-7ed5-4906-843b-41355a7d9dc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6938f2e8-7ed5-4906-843b-41355a7d9dc8",
    "outputId": "15826743-8d51-41f7-dcab-0b3504f038a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the character embedding\n",
    "import chardet\n",
    "with open(path_Scholar, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42819315-b087-44a2-84d3-48d4e7d259c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42819315-b087-44a2-84d3-48d4e7d259c1",
    "outputId": "94824e16-e8e3-4507-9888-9551df05d664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'ISO-8859-1', 'confidence': 0.7298232262081848, 'language': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chardet\n",
    "with open(path_DBLP, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41fa2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "8e41fa2d",
    "outputId": "6c740c28-7f47-4e3e-a32f-ad5f881c1cfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aKcZKwvwbQwJ</td>\n",
       "      <td>11578 sorrento valley road</td>\n",
       "      <td>QD Inc</td>\n",
       "      <td>San Diego,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ixKfiTHoaDoJ</td>\n",
       "      <td>initiation of crazes in polystyrene</td>\n",
       "      <td>AS Argon, JG Hannoosh</td>\n",
       "      <td>Phil. Mag,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3BxllB4wwcIJ</td>\n",
       "      <td>immunogold labelling is a quantitative method as demonstrated by studies on aminopeptidase n in</td>\n",
       "      <td>GH Hansen, LL Wetterberg, H SjÃ¶strÃ¶m, O NorÃ©n</td>\n",
       "      <td>The Histochemical Journal,</td>\n",
       "      <td>1992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d2WWxwKMex4J</td>\n",
       "      <td>the burden of infectious disease among inmates and releasees from correctional facilities</td>\n",
       "      <td>TM Hammett, P Harmon, W Rhodes</td>\n",
       "      <td>see</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cZCX-AQpjccJ</td>\n",
       "      <td>the role of faculty advising in science and engineering</td>\n",
       "      <td>JR Cogdell</td>\n",
       "      <td>NEW DIRECTIONS FOR TEACHING AND LEARNING,</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ...    year\n",
       "0  aKcZKwvwbQwJ  ...     NaN\n",
       "1  ixKfiTHoaDoJ  ...     NaN\n",
       "2  3BxllB4wwcIJ  ...  1992.0\n",
       "3  d2WWxwKMex4J  ...     NaN\n",
       "4  cZCX-AQpjccJ  ...  1995.0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Scholar data table\n",
    "Scholar = em.read_csv_metadata(path_Scholar, key='id')\n",
    "# Transforming the title column cells\n",
    "Scholar['title'] = Scholar['title'].str.lower()\n",
    "Scholar['title'] = Scholar['title'].str.replace('[^A-Za-z0-9 ]+', '')\n",
    "Scholar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd35b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "ff0bd35b",
    "outputId": "6a031ce6-48d9-4c54-ad49-522aa7ace8ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conf/vldb/RusinkiewiczKTWM95</td>\n",
       "      <td>towards a cooperative transaction model  the cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J W�sch, P Muth</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>journals/sigmod/EisenbergM02</td>\n",
       "      <td>sqlxml is making good progress</td>\n",
       "      <td>A Eisenberg, J Melton</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/vldb/AmmannJR95</td>\n",
       "      <td>using formal methods to reason about semanticsbased decompositions of transactions</td>\n",
       "      <td>P Ammann, S Jajodia, I Ray</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>journals/sigmod/Liu02</td>\n",
       "      <td>editors notes</td>\n",
       "      <td>L Liu</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>journals/sigmod/Hammer02</td>\n",
       "      <td>report on the acm fourth international workshop on data warehousing and olap dolap 2001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  ...  year\n",
       "0  conf/vldb/RusinkiewiczKTWM95  ...  1995\n",
       "1  journals/sigmod/EisenbergM02  ...  2002\n",
       "2          conf/vldb/AmmannJR95  ...  1995\n",
       "3         journals/sigmod/Liu02  ...  2002\n",
       "4      journals/sigmod/Hammer02  ...  2002\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBLP = em.read_csv_metadata(path_DBLP, key='id')\n",
    "DBLP['title'] = DBLP['title'].str.replace('[^A-Za-z0-9 ]+', '')\n",
    "DBLP['title'] = DBLP['title'].str.lower()\n",
    "DBLP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cT3uVZtBjiv_",
   "metadata": {
    "id": "cT3uVZtBjiv_"
   },
   "source": [
    "# Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308c287",
   "metadata": {
    "id": "a308c287"
   },
   "outputs": [],
   "source": [
    "# Get tokenizers and similarity function\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "\n",
    "# Get attributes\n",
    "atypes1 = em.get_attr_types(Scholar)\n",
    "atypes2 = em.get_attr_types(DBLP)\n",
    "\n",
    "# Get correspondence\n",
    "block_c = em.get_attr_corres(Scholar, DBLP)\n",
    "\n",
    "#Get Features\n",
    "block_f = em.get_features(Scholar, DBLP, atypes1, atypes2, block_c, block_t, block_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4984c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4984c3",
    "outputId": "4e2a4d3e-4557-4c34-cb7f-830587bd114e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding pairs with missing value...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:34\n"
     ]
    }
   ],
   "source": [
    "# Creating a rule based blocker\n",
    "rb = em.RuleBasedBlocker()\n",
    "# 0.41 achieves 49% balance negative to positive\n",
    "rule = ['title_title_jac_qgm_3_qgm_3(ltuple, rtuple) < 0.41']\n",
    "rb.add_rule(rule, feature_table=block_f)\n",
    "C = rb.block_tables(Scholar, DBLP,\n",
    "                   l_output_attrs=['title', 'authors', 'venue', 'year'], \n",
    "                   r_output_attrs=['title', 'authors', 'venue', 'year'], \n",
    "                   n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b5b35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f4b5b35",
    "outputId": "42d60f05-10ae-46e8-abd0-3fc154f99c96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10693"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of tuple pairs in D.\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba3419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "ddba3419",
    "outputId": "25bfec68-23cc-4a52-81cc-566c809c26d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>ltable_title</th>\n",
       "      <th>ltable_authors</th>\n",
       "      <th>ltable_venue</th>\n",
       "      <th>ltable_year</th>\n",
       "      <th>rtable_title</th>\n",
       "      <th>rtable_authors</th>\n",
       "      <th>rtable_venue</th>\n",
       "      <th>rtable_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>019zSr3Lx4EJ</td>\n",
       "      <td>conf/vldb/RusinkiewiczKTWM95</td>\n",
       "      <td>towards a cooperative activity modelthe coopertive activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J Wasch, P Muth</td>\n",
       "      <td>Proceedings of the 21st International Conference on Very  &amp;hellip;,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>towards a cooperative transaction model  the cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J W�sch, P Muth</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>KureLsKbUXYJ</td>\n",
       "      <td>conf/vldb/RusinkiewiczKTWM95</td>\n",
       "      <td>towaxds a cooperative transaction modelthe cooperative activity model</td>\n",
       "      <td>M Rnsinkiewicz, W Klasâ?¦</td>\n",
       "      <td>Proc. Int. Conf. Very Large Data Bases, Dayal U, Gray P,  &amp;hellip;,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>towards a cooperative transaction model  the cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J W�sch, P Muth</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>T2fm7Wb1ak4J</td>\n",
       "      <td>conf/vldb/RusinkiewiczKTWM95</td>\n",
       "      <td>towards a cooperative transaction modelthe cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J Waesch, P Muth</td>\n",
       "      <td>PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE  &amp;hellip;,</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>towards a cooperative transaction model  the cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J W�sch, P Muth</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>iWNLOYCQX-YJ</td>\n",
       "      <td>conf/vldb/RusinkiewiczKTWM95</td>\n",
       "      <td>w asch j muth p towards a cooperative transaction modelthe cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch</td>\n",
       "      <td>Proc. ofthe 21 stInternational Conference on Very Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>towards a cooperative transaction model  the cooperative activity model</td>\n",
       "      <td>M Rusinkiewicz, W Klas, T Tesch, J W�sch, P Muth</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wgK6p4mDSIMJ</td>\n",
       "      <td>journals/sigmod/EisenbergM02</td>\n",
       "      <td>sqlxml is making good progress</td>\n",
       "      <td>A Eisenberg, J Melton</td>\n",
       "      <td>SIGMOD Record,</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>sqlxml is making good progress</td>\n",
       "      <td>A Eisenberg, J Melton</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id     ltable_id  ...   rtable_venue rtable_year\n",
       "0    0  019zSr3Lx4EJ  ...           VLDB        1995\n",
       "1    1  KureLsKbUXYJ  ...           VLDB        1995\n",
       "2    2  T2fm7Wb1ak4J  ...           VLDB        1995\n",
       "3    3  iWNLOYCQX-YJ  ...           VLDB        1995\n",
       "4    4  wgK6p4mDSIMJ  ...  SIGMOD Record        2002\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f449321-075c-4ce3-8a64-316df8c339bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f449321-075c-4ce3-8a64-316df8c339bd",
    "outputId": "5f443ef7-aeae-4211-aa12-431c22e875d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled pairs: 5347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5347, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The path to the labeled data file.\n",
    "path_Match = os.path.join('.', 'Dissertation_Data_2', 'DBLP-Scholar_Perfect_Mapping.csv')\n",
    "# Load the labeled data into a dataframe.\n",
    "M = em.read_csv_metadata(path_Match, \n",
    "                         ltable=Scholar, rtable=DBLP, \n",
    "                         fk_ltable='idScholar', fk_rtable='idDBLP')\n",
    "print('Number of labeled pairs:', len(M))\n",
    "L = em.read_csv_metadata(path_Match)\n",
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7d4ad-5deb-4caa-9a14-f86c89a19eda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abc7d4ad-5deb-4caa-9a14-f86c89a19eda",
    "outputId": "4167c469-bf54-44ca-9053-eeb017c3e54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5347, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b81c4-ea61-4980-8ebd-e66529b1c6ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "c13b81c4-ea61-4980-8ebd-e66529b1c6ed",
    "outputId": "467d7738-ef29-4164-f4ec-caa693ccf639"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idDBLP</th>\n",
       "      <th>idScholar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conf/sigmod/AbadiC02</td>\n",
       "      <td>f2Lea-RN8dsJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf/sigmod/AbadiCCCCEGHMRSSTXYZ03</td>\n",
       "      <td>eBnT7lhV2LwJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/sigmod/AbadiCCCCEGHMRSSTXYZ03</td>\n",
       "      <td>gBVNSFeS4P8J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf/sigmod/AbadiCCCCEGHMRSSTXYZ03</td>\n",
       "      <td>VuY9Y49GqXgJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf/sigmod/AbiteboulBCMM03</td>\n",
       "      <td>AxpQwgyRyLgJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               idDBLP     idScholar\n",
       "0                conf/sigmod/AbadiC02  f2Lea-RN8dsJ\n",
       "1  conf/sigmod/AbadiCCCCEGHMRSSTXYZ03  eBnT7lhV2LwJ\n",
       "2  conf/sigmod/AbadiCCCCEGHMRSSTXYZ03  gBVNSFeS4P8J\n",
       "3  conf/sigmod/AbadiCCCCEGHMRSSTXYZ03  VuY9Y49GqXgJ\n",
       "4         conf/sigmod/AbiteboulBCMM03  AxpQwgyRyLgJ"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lm6AXcXtjpPo",
   "metadata": {
    "id": "lm6AXcXtjpPo"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb8f13-9933-4115-8c20-1610c51d3f64",
   "metadata": {
    "id": "97cb8f13-9933-4115-8c20-1610c51d3f64"
   },
   "outputs": [],
   "source": [
    "def labelling(train, labelled, key = \"idDBLP\", val = \"idScholar\"):\n",
    "  '''\n",
    "  Create dictionary for checking how many matches are retained and perform labelling\n",
    "  Input:\n",
    "  - train (pd.DataFrame): DataFrame object of blocked data used for training\n",
    "  - lbelled (pd.DataFrame): The labelled DataFrame for labelling the train data\n",
    "  - key (string): which table id in blocked \"train\" to use as key for labelling\n",
    "  - val (string): which table id in blocked \"train\" to use as value for labelling\n",
    "  Output:\n",
    "  - train (pd.DataFrame): The labelled training dataframe\n",
    "  '''\n",
    "  # merge id columns in one to uniquely identify\n",
    "  train[\"id_comb\"] = train[\"rtable_id\"].astype(str) + train[\"ltable_id\"].astype(str)\n",
    "  labelled[\"id_comb\"] = labelled[key].astype(str) + labelled[val].astype(str)\n",
    "  # merge the training and labelled dataset and assign labels\n",
    "  K = train.merge(labelled, on='id_comb', how='left', indicator=True)\n",
    "  K.loc[(K['_merge'] == \"both\"), 'label'] = 1\n",
    "  K.loc[(K['_merge'] == \"left_only\") | (K['_merge'] == \"right_only\"), 'label'] = 0\n",
    "  # delete unused columns\n",
    "  del K['_merge']\n",
    "  del K[key]\n",
    "  del K[val]\n",
    "  del K['id_comb']\n",
    "\n",
    "  # cleaning the data\n",
    "  for col in K.columns:\n",
    "    if 'id' not in col and col != \"label\":\n",
    "      K[col] = K[col].astype(str).str.replace('[^A-Za-z0-9 ]+', '')\n",
    "  \n",
    "  if key == \"id1\":\n",
    "      K['_id'] = K['_id'].astype(int)\n",
    "  K['label'] = K['label'].astype(int)\n",
    "  return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KwpT-nyHqZKE",
   "metadata": {
    "id": "KwpT-nyHqZKE"
   },
   "outputs": [],
   "source": [
    "def scores(time, predictions, iteration, train_n, model, epochs=15):\n",
    "  '''\n",
    "  Report the most important metrics of performance from the predictions made by the model\n",
    "  Input:\n",
    "  - time (float): time the model ran for\n",
    "  - predictions (pd.DataFrame): contains the output features and column \"match_score\", containing the prediction information\n",
    "  - iteration (int64): the current iteration if applicable\n",
    "  - train_n (int64): the number of samples used in training\n",
    "  - model (string): the model that was used\n",
    "  - epochs (int64): the number of epochs the model was trained for\n",
    "  Output:\n",
    "  - dict (dictionary): dictionary containing the performance information  \n",
    "  '''\n",
    "  # casting prediction scores as binary labels\n",
    "  predictions['match_prediction'] = predictions['match_score'].apply(lambda score: 1 if score >= 0.5 else 0)\n",
    "  # Reset index as Magellan requires the key to be a column in the table\n",
    "  predictions.reset_index(inplace=True)\n",
    "  # Update metadata in the catalog. This information can later be used by triggers to modify the labels from \n",
    "  # the learning-based matcher \n",
    "  em.set_key(predictions, '_id')\n",
    "  em.set_fk_ltable(predictions, 'ltable_id')\n",
    "  em.set_fk_rtable(predictions, 'rtable_id')\n",
    "  # AUC-ROC metrics\n",
    "  auc = sk.metrics.roc_auc_score(predictions['label'], predictions['match_score'], average = None)\n",
    "  # Precision Metrics\n",
    "  prec = em.eval_matches(predictions, 'label', 'match_prediction')\n",
    "  # Creating dictionary of performance metrics to append to a DataFrame\n",
    "  dict = {'Model':model, 'Time':time, 'Epochs':epochs,'Iteration':iteration, '#Training_samples':train_n,\n",
    "                          'F1':prec['f1'], 'Precision':prec['precision'], \n",
    "                          'Recall':prec['recall'], 'AUC':auc}\n",
    "  return dict\n",
    "# Create a placeholder matrix to append precision scores to\n",
    "prec_matrix = pd.DataFrame(columns=['Model', 'Time', 'Epochs', 'Iteration', '#Training_samples', 'F1', 'Precision', 'Recall', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H3hDpyxSgSQ-",
   "metadata": {
    "id": "H3hDpyxSgSQ-"
   },
   "outputs": [],
   "source": [
    "def grid_search_block(dataL = Scholar, dataR = DBLP, matched = L):\n",
    "  '''\n",
    "  Checking the balance between positive and negative matches and the positive \n",
    "  matches retained by gridsearch.\n",
    "  Input:\n",
    "  - dataL (pandas DataFrame): Data Frame containing the left table\n",
    "  - dataR (pandas DataFrame): Data Frame containing the right table\n",
    "  - L (pandas DataFrame): Data Frame containing the actual matches\n",
    "\n",
    "  Output:\n",
    "  - results (matrix): matrix containing balance and retained matches information\n",
    "  '''\n",
    "  # Get tokenizers and similarity function\n",
    "  block_t = em.get_tokenizers_for_blocking()\n",
    "  block_s = em.get_sim_funs_for_blocking()\n",
    "  # Get attributes\n",
    "  atypes1 = em.get_attr_types(Scholar)\n",
    "  atypes2 = em.get_attr_types(DBLP)\n",
    "  # Get correspondence\n",
    "  block_c = em.get_attr_corres(Scholar, DBLP)\n",
    "  #Get Features\n",
    "  block_f = em.get_features(dataL, dataR, atypes1, atypes2, block_c, block_t, block_s)\n",
    "  # result data frame\n",
    "  results = np.zeros((21, 2))\n",
    "  i = 0\n",
    "  # grid search\n",
    "  for thr in np.linspace(0.5, 0.3, 21):\n",
    "    print(thr)\n",
    "    # Creating a rule based blocker\n",
    "    rb = em.RuleBasedBlocker()\n",
    "    rule = [f'title_title_jac_qgm_3_qgm_3(ltuple, rtuple) < {thr}']\n",
    "    rb.add_rule(rule, feature_table=block_f)\n",
    "    l_attr = list(dataL.columns)\n",
    "    r_attr = list(dataR.columns)\n",
    "    C = rb.block_tables(dataL, dataR,\n",
    "                      l_output_attrs=l_attr, \n",
    "                        r_output_attrs=r_attr, \n",
    "                      n_jobs=1)\n",
    "    \n",
    "    # create dictionary for checking how many matches are retained\n",
    "    mapping = defaultdict(lambda: [])\n",
    "    for row in C.iterrows():\n",
    "      mapping[row[1][\"rtable_id\"]].append(row[1][\"ltable_id\"])\n",
    "\n",
    "    ct = 0\n",
    "    for row in L.iterrows():\n",
    "      id_dbl = row[1][\"idDBLP\"]\n",
    "      id_sch = row[1][\"idScholar\"]\n",
    "      if id_sch in mapping[id_dbl]:\n",
    "        ct += 1\n",
    "\n",
    "    results[i, 0] = round(ct / L.shape[0], 2)\n",
    "    results[i, 1] = round(ct / C.shape[0], 2)\n",
    "    i += 1\n",
    "  # results.columns = [\"PercRet\", \"PosNegRat\"]\n",
    "  \n",
    "  return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HNpI3iflgRc-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNpI3iflgRc-",
    "outputId": "04553ccb-7de6-4cac-e8c8-d513e3134a69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of matches retained after blocking: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Labelling the blocked data set\n",
    "cand = labelling(C, L, key = \"idDBLP\", val = \"idScholar\")\n",
    "print(\"Proportion of matches retained after blocking:\", round(sum(cand['label']) / L.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qoQg_pbxRK6N",
   "metadata": {
    "id": "qoQg_pbxRK6N"
   },
   "outputs": [],
   "source": [
    "# saving and loading the candidate set\n",
    "path_cand = os.path.join('.', 'Dissertation_Data_2', 'cand.csv')\n",
    "cand.to_csv(path_cand, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g6a99nYEbD6X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6a99nYEbD6X",
    "outputId": "c9c71f26-9c74-4c6b-d971-f3a3effed78c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'ltable_id', 'rtable_id', 'ltable_title', 'ltable_authors',\n",
       "       'ltable_venue', 'ltable_year', 'rtable_title', 'rtable_authors',\n",
       "       'rtable_venue', 'rtable_year', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand['label'] = cand['label'].astype(int)\n",
    "cand.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qdbqXuEYnSca",
   "metadata": {
    "id": "qdbqXuEYnSca"
   },
   "source": [
    "# Processing and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4S3ref55nfdd",
   "metadata": {
    "id": "4S3ref55nfdd"
   },
   "source": [
    "## DL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d_Vfv2Kk3-iE",
   "metadata": {
    "id": "d_Vfv2Kk3-iE"
   },
   "outputs": [],
   "source": [
    "# The directory where the data splits will be saved.\n",
    "split_path = os.path.join('.', 'Dissertation_Data_2')\n",
    "# Split labeled data into train, valid, and test csv files to disk, with the split ratio of 3:1:1.\n",
    "dm.data.split(cand, split_path, 'train.csv', 'valid.csv', 'test.csv',\n",
    "              [3, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UW2lMkvm4Qbu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW2lMkvm4Qbu",
    "outputId": "8a112173-6ed1-45e8-ef96-432f521773fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and processing data from \"./Dissertation_Data_2/train.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"./Dissertation_Data_2/valid.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"./Dissertation_Data_2/test.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00INFO:deepmatcher.data.field:Downloading vectors from https://drive.google.com/uc?export=download&id=1Vih8gAmgBnuYDxfblbT94P6WjB7s1ZSh to /root/.vector_cache/wiki.en.bin\n",
      "INFO:deepmatcher.data.field:Unable to fetch cached English Word Embeddings from https://drive.google.com/uc?export=download&id=1Vih8gAmgBnuYDxfblbT94P6WjB7s1ZSh\n",
      "Downloading embeddings from https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip to /root/.vector_cache/wiki.en.zip\n",
      "/usr/local/lib/python3.7/dist-packages/deepmatcher/data/field.py:79: ResourceWarning: unclosed <ssl.SSLSocket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 44452), raddr=('108.177.119.102', 443)>\n",
      "  self.destination = self.backup_destination\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:deepmatcher.data.field:Extracting vectors into /root/.vector_cache\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "\n",
      "Building vocabulary\n",
      "0% [#######] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n",
      "\n",
      "Computing principal components\n",
      "0% [#######] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "# Load the training data files from the disk. Ignore the \"left_id\" and \"right_id\" \n",
    "# columns for data preprocessing.\n",
    "# The 'use_magellan_convention' parameter asks deepmatcher to use Magellan's \n",
    "# naming convention for the left and right table column name prefixes \n",
    "# (\"ltable_\", and \"rtable_\"), and also to consider \"_id\" as the ID column.\n",
    "train, validation, test = dm.data.process(\n",
    "    path=os.path.join('.', 'Dissertation_Data_2'),\n",
    "    cache='train_cache.pth',\n",
    "    train='train.csv',\n",
    "    validation='valid.csv',\n",
    "    test='test.csv',\n",
    "    use_magellan_convention=True,\n",
    "    ignore_columns=('ltable_id', 'rtable_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CXJ3v9MULAQL",
   "metadata": {
    "id": "CXJ3v9MULAQL"
   },
   "source": [
    "### Sif-Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EgwbpBsx4mkJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgwbpBsx4mkJ",
    "outputId": "47f9ec4d-bfdd-4389-dba2-c41f914d962f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 542402\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    6.5 | Load Time:    1.4 || F1:  78.41 | Prec:  86.29 | Rec:  71.84 || Ex/s: 814.24\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    1.1 | Load Time:    0.5 || F1:  89.31 | Prec:  87.03 | Rec:  91.71 || Ex/s: 1370.59\n",
      "\n",
      "* Best F1: tensor(89.3070)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    6.4 | Load Time:    1.4 || F1:  93.31 | Prec:  91.53 | Rec:  95.17 || Ex/s: 826.52\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.59 | Prec:  90.82 | Rec:  92.37 || Ex/s: 1375.18\n",
      "\n",
      "* Best F1: tensor(91.5888)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    6.2 | Load Time:    1.4 || F1:  96.34 | Prec:  95.36 | Rec:  97.33 || Ex/s: 841.02\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    1.0 | Load Time:    0.5 || F1:  91.33 | Prec:  90.32 | Rec:  92.37 || Ex/s: 1409.75\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    6.3 | Load Time:    1.4 || F1:  97.77 | Prec:  97.30 | Rec:  98.24 || Ex/s: 837.83\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.66 | Prec:  90.15 | Rec:  93.21 || Ex/s: 1344.74\n",
      "\n",
      "* Best F1: tensor(91.6589)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    6.4 | Load Time:    1.4 || F1:  98.36 | Prec:  98.04 | Rec:  98.68 || Ex/s: 822.73\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.70 | Prec:  89.79 | Rec:  93.69 || Ex/s: 1376.26\n",
      "\n",
      "* Best F1: tensor(91.6974)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    6.3 | Load Time:    1.4 || F1:  98.76 | Prec:  98.50 | Rec:  99.03 || Ex/s: 827.51\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.60 | Prec:  89.77 | Rec:  93.50 || Ex/s: 1364.68\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.12 | Prec:  98.97 | Rec:  99.28 || Ex/s: 835.02\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    1.0 | Load Time:    0.5 || F1:  91.54 | Prec:  89.84 | Rec:  93.31 || Ex/s: 1406.09\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.33 | Prec:  99.22 | Rec:  99.44 || Ex/s: 837.28\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    1.0 | Load Time:    0.5 || F1:  91.61 | Prec:  90.15 | Rec:  93.12 || Ex/s: 1399.49\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.45 | Prec:  99.37 | Rec:  99.53 || Ex/s: 834.06\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    1.0 | Load Time:    0.5 || F1:  91.53 | Prec:  90.36 | Rec:  92.74 || Ex/s: 1398.85\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    6.2 | Load Time:    1.4 || F1:  99.50 | Prec:  99.44 | Rec:  99.56 || Ex/s: 839.72\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.50 | Prec:  90.73 | Rec:  92.27 || Ex/s: 1398.23\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.55 | Prec:  99.50 | Rec:  99.59 || Ex/s: 837.45\n",
      "\n",
      "===>  EVAL Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:    1.0 | Load Time:    0.5 || F1:  91.37 | Prec:  90.94 | Rec:  91.80 || Ex/s: 1393.77\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:    6.4 | Load Time:    1.4 || F1:  99.58 | Prec:  99.53 | Rec:  99.62 || Ex/s: 824.93\n",
      "\n",
      "===>  EVAL Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.50 | Prec:  91.20 | Rec:  91.80 || Ex/s: 1362.86\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.66 | Prec:  99.62 | Rec:  99.69 || Ex/s: 830.79\n",
      "\n",
      "===>  EVAL Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.43 | Prec:  91.35 | Rec:  91.52 || Ex/s: 1377.43\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.70 | Prec:  99.69 | Rec:  99.72 || Ex/s: 833.26\n",
      "\n",
      "===>  EVAL Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.42 | Prec:  91.50 | Rec:  91.33 || Ex/s: 1374.79\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:    6.3 | Load Time:    1.4 || F1:  99.70 | Prec:  99.69 | Rec:  99.72 || Ex/s: 832.94\n",
      "\n",
      "===>  EVAL Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:    1.1 | Load Time:    0.5 || F1:  91.44 | Prec:  91.75 | Rec:  91.14 || Ex/s: 1375.83\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    0.7 | Load Time:    0.5 || F1:  90.54 | Prec:  88.34 | Rec:  92.86 || Ex/s: 1895.87\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(90.5412)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sifDiff model.\n",
    "model_sifdiff = dm.MatchingModel(attr_summarizer='sif')\n",
    "model_sifdiff.initialize(train)  # Initilization\n",
    "\n",
    "# Train ing model on 15 epochs, batch size of 16, positive-to-negative \n",
    "# ratio to be 1:1. We save the best model (with the \n",
    "# highest F1 score on the validation set) to 'sifDiff_model.pth'\n",
    "startTime = time.time()\n",
    "model_sifdiff.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    best_save_path='sifDiff_model.pth')\n",
    "executionTime_sifDiff = (time.time() - startTime)/60\n",
    "\n",
    "# Evaluate the accuracy on the test data.\n",
    "model_sifdiff.run_eval(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nx4psuTJA1TJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nx4psuTJA1TJ",
    "outputId": "6447fc1d-6625-466a-e046-fe814f620d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    0.7 | Load Time:    0.5 || F1:  90.54 | Prec:  88.34 | Rec:  92.86 || Ex/s: 1791.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "model_sifdiff = dm.MatchingModel(attr_summarizer='sif')\n",
    "model_sifdiff.load_state('sifDiff_model.pth')\n",
    "# Load the candidate set. Note that the trained model is an input parameter as we need to trained \n",
    "# model for candidate set preprocessing.\n",
    "predictions_sifDiff = model_sifdiff.run_prediction(test, output_attributes=list(test.get_raw_table().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K_grXqn0qeVz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_grXqn0qeVz",
    "outputId": "ea8eb29d-0366-40cd-85d0-d55fbf8121e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "# get training data set and compute performance scores\n",
    "train_n = em.read_csv_metadata(os.path.join('.', 'Dissertation_Data_2', 'train.csv'))\n",
    "score_sifDiff = scores(executionTime_sifDiff, predictions_sifDiff, 0, train_n.shape[0], model = 'sif_diff')\n",
    "prec_matrix = prec_matrix.append(score_sifDiff, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UvZqFPuKCufj",
   "metadata": {
    "id": "UvZqFPuKCufj"
   },
   "outputs": [],
   "source": [
    "high_score_pairs = predictions_sifDiff[predictions_sifDiff['match_score'] >= 0.9].sort_values(by=['match_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbxWDBzAC9yB",
   "metadata": {
    "id": "HbxWDBzAC9yB"
   },
   "outputs": [],
   "source": [
    "predictions_sifDiff['match_prediction'] = predictions_sifDiff['match_score'].apply(lambda score: 1 if score >= 0.5 else 0)\n",
    "# Reorder columns to avoid scrolling...\n",
    "predictions_sifDiff =  predictions_sifDiff[['match_score', 'match_prediction'] + predictions_sifDiff.columns.values[1:-1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dcj6JA6llUto",
   "metadata": {
    "id": "Dcj6JA6llUto"
   },
   "outputs": [],
   "source": [
    "path_predictions = os.path.join('.', 'Dissertation_Data_2', 'predictions_sifDiff.csv')\n",
    "predictions_sifDiff.to_csv(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WdIPDct3LQTj",
   "metadata": {
    "id": "WdIPDct3LQTj"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z2akfSBlLPlh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2akfSBlLPlh",
    "outputId": "24eaa21d-a883-4e74-c689-af4f5a6dd7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 2169602\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:949: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:939: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:   62.8 | Load Time:    1.5 || F1:  87.67 | Prec:  90.30 | Rec:  85.20 || Ex/s:  99.85\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    8.3 | Load Time:    0.5 || F1:  89.77 | Prec:  92.94 | Rec:  86.80 || Ex/s: 243.29\n",
      "\n",
      "* Best F1: tensor(89.7661)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:   62.4 | Load Time:    1.4 || F1:  96.12 | Prec:  96.00 | Rec:  96.24 || Ex/s: 100.44\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    8.2 | Load Time:    0.5 || F1:  90.81 | Prec:  95.43 | Rec:  86.62 || Ex/s: 246.46\n",
      "\n",
      "* Best F1: tensor(90.8103)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   62.0 | Load Time:    1.4 || F1:  98.23 | Prec:  98.24 | Rec:  98.21 || Ex/s: 101.09\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    8.0 | Load Time:    0.5 || F1:  92.20 | Prec:  93.59 | Rec:  90.86 || Ex/s: 250.32\n",
      "\n",
      "* Best F1: tensor(92.2047)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:   61.9 | Load Time:    1.4 || F1:  98.90 | Prec:  98.84 | Rec:  98.97 || Ex/s: 101.38\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    8.1 | Load Time:    0.5 || F1:  92.81 | Prec:  92.51 | Rec:  93.12 || Ex/s: 248.10\n",
      "\n",
      "* Best F1: tensor(92.8135)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:   62.0 | Load Time:    1.4 || F1:  99.17 | Prec:  99.22 | Rec:  99.12 || Ex/s: 101.12\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    8.1 | Load Time:    0.5 || F1:  92.82 | Prec:  91.87 | Rec:  93.78 || Ex/s: 247.16\n",
      "\n",
      "* Best F1: tensor(92.8172)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   61.8 | Load Time:    1.4 || F1:  99.37 | Prec:  99.40 | Rec:  99.34 || Ex/s: 101.41\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    8.2 | Load Time:    0.5 || F1:  93.17 | Prec:  90.64 | Rec:  95.85 || Ex/s: 246.48\n",
      "\n",
      "* Best F1: tensor(93.1745)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:   62.1 | Load Time:    1.4 || F1:  99.59 | Prec:  99.59 | Rec:  99.59 || Ex/s: 101.00\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    8.2 | Load Time:    0.5 || F1:  93.63 | Prec:  91.16 | Rec:  96.23 || Ex/s: 244.82\n",
      "\n",
      "* Best F1: tensor(93.6268)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:   65.9 | Load Time:    1.5 || F1:  99.67 | Prec:  99.66 | Rec:  99.69 || Ex/s:  95.14\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    9.4 | Load Time:    0.6 || F1:  93.51 | Prec:  91.37 | Rec:  95.76 || Ex/s: 214.01\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:   66.7 | Load Time:    1.6 || F1:  99.73 | Prec:  99.75 | Rec:  99.72 || Ex/s:  93.98\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    8.2 | Load Time:    0.5 || F1:  93.64 | Prec:  92.31 | Rec:  95.00 || Ex/s: 245.23\n",
      "\n",
      "* Best F1: tensor(93.6368)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:   62.9 | Load Time:    1.4 || F1:  99.80 | Prec:  99.81 | Rec:  99.78 || Ex/s:  99.72\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    8.4 | Load Time:    0.5 || F1:  93.38 | Prec:  92.35 | Rec:  94.44 || Ex/s: 241.00\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:   63.4 | Load Time:    1.4 || F1:  99.81 | Prec:  99.84 | Rec:  99.78 || Ex/s:  98.94\n",
      "\n",
      "===>  EVAL Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:    8.3 | Load Time:    0.5 || F1:  93.41 | Prec:  92.59 | Rec:  94.25 || Ex/s: 242.15\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:   63.5 | Load Time:    1.4 || F1:  99.86 | Prec:  99.91 | Rec:  99.81 || Ex/s:  98.86\n",
      "\n",
      "===>  EVAL Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:    8.3 | Load Time:    0.5 || F1:  93.51 | Prec:  92.69 | Rec:  94.34 || Ex/s: 242.62\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:   63.3 | Load Time:    1.4 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  99.16\n",
      "\n",
      "===>  EVAL Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:    8.3 | Load Time:    0.5 || F1:  93.56 | Prec:  92.69 | Rec:  94.44 || Ex/s: 242.12\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:   62.7 | Load Time:    1.4 || F1:  99.86 | Prec:  99.91 | Rec:  99.81 || Ex/s: 100.11\n",
      "\n",
      "===>  EVAL Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:    8.3 | Load Time:    0.5 || F1:  93.60 | Prec:  92.78 | Rec:  94.44 || Ex/s: 243.29\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:   63.2 | Load Time:    1.4 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  99.26\n",
      "\n",
      "===>  EVAL Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:    8.3 | Load Time:    0.5 || F1:  93.60 | Prec:  92.78 | Rec:  94.44 || Ex/s: 243.37\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    6.8 | Load Time:    0.5 || F1:  94.37 | Prec:  94.06 | Rec:  94.69 || Ex/s: 293.48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(94.3723)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sifDiff model.\n",
    "model_rnn = dm.MatchingModel(attr_summarizer='rnn')\n",
    "model_rnn.initialize(train)  # Initilization\n",
    "\n",
    "# Train ing model on 15 epochs, batch size of 16, positive-to-negative \n",
    "# ratio to be 1:1. We save the best model (with the \n",
    "# highest F1 score on the validation set) to 'sifDiff_model.pth'.\n",
    "startTime = time.time()\n",
    "model_rnn.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    best_save_path='rnn_model.pth')\n",
    "executionTime_rnn = (time.time() - startTime)/60\n",
    "\n",
    "# Evaluate the accuracy on the test data.\n",
    "model_rnn.run_eval(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQk85eYALZJn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQk85eYALZJn",
    "outputId": "597cc33f-2b89-48c0-faab-b37617c3bf23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    6.9 | Load Time:    0.5 || F1:  94.37 | Prec:  94.06 | Rec:  94.69 || Ex/s: 289.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "model_rnn = dm.MatchingModel(attr_summarizer='rnn')\n",
    "model_rnn.load_state('rnn_model.pth')\n",
    "# Load the candidate set. Note that the trained model is an input parameter as we need to trained \n",
    "# model for candidate set preprocessing.\n",
    "predictions_rnn = model_rnn.run_prediction(test, output_attributes=list(test.get_raw_table().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R9ZaNkqWMpnA",
   "metadata": {
    "id": "R9ZaNkqWMpnA"
   },
   "outputs": [],
   "source": [
    "# get training data set and compute performance scores\n",
    "score_rnn = scores(executionTime_rnn, predictions_rnn, 0, train_n.shape[0], model = 'rnn')\n",
    "prec_matrix = prec_matrix.append(score_rnn, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RjMAW2GqM2Ax",
   "metadata": {
    "id": "RjMAW2GqM2Ax"
   },
   "outputs": [],
   "source": [
    "high_score_pairs = predictions_rnn[predictions_rnn['match_score'] >= 0.9].sort_values(by=['match_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HeWe9llnM3n0",
   "metadata": {
    "id": "HeWe9llnM3n0"
   },
   "outputs": [],
   "source": [
    "predictions_rnn['match_prediction'] = predictions_rnn['match_score'].apply(lambda score: 1 if score >= 0.5 else 0)\n",
    "# Reorder columns to avoid scrolling...\n",
    "predictions_rnn =  predictions_rnn[['match_score', 'match_prediction'] + predictions_rnn.columns.values[1:-1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tgN7zL3IlN6S",
   "metadata": {
    "id": "tgN7zL3IlN6S"
   },
   "outputs": [],
   "source": [
    "path_predictions = os.path.join('.', 'Dissertation_Data_2', 'predictions_rnn.csv')\n",
    "predictions_rnn.to_csv(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROYiORgGNY_x",
   "metadata": {
    "id": "ROYiORgGNY_x"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qZe6QSfFNeFX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZe6QSfFNeFX",
    "outputId": "6e364643-bdee-48ac-f65c-60f45e7f4ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 4332002\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:949: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:939: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:  138.5 | Load Time:    1.4 || F1:  88.92 | Prec:  87.78 | Rec:  90.09 || Ex/s:  45.84\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:   17.5 | Load Time:    0.5 || F1:  92.85 | Prec:  91.49 | Rec:  94.25 || Ex/s: 118.80\n",
      "\n",
      "* Best F1: tensor(92.8505)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:  139.8 | Load Time:    1.4 || F1:  94.43 | Prec:  93.27 | Rec:  95.61 || Ex/s:  45.42\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:   17.4 | Load Time:    0.5 || F1:  93.98 | Prec:  93.80 | Rec:  94.16 || Ex/s: 119.21\n",
      "\n",
      "* Best F1: tensor(93.9793)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:  137.4 | Load Time:    1.5 || F1:  96.59 | Prec:  96.29 | Rec:  96.90 || Ex/s:  46.22\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   17.6 | Load Time:    0.5 || F1:  93.93 | Prec:  95.25 | Rec:  92.65 || Ex/s: 118.19\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:  140.2 | Load Time:    1.4 || F1:  97.58 | Prec:  97.32 | Rec:  97.84 || Ex/s:  45.31\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:   17.5 | Load Time:    0.5 || F1:  93.96 | Prec:  94.90 | Rec:  93.03 || Ex/s: 118.93\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:  140.0 | Load Time:    1.4 || F1:  98.28 | Prec:  98.18 | Rec:  98.37 || Ex/s:  45.36\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:   17.5 | Load Time:    0.5 || F1:  93.46 | Prec:  94.68 | Rec:  92.27 || Ex/s: 118.79\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:  141.1 | Load Time:    1.4 || F1:  98.89 | Prec:  98.99 | Rec:  98.78 || Ex/s:  45.01\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   17.4 | Load Time:    0.5 || F1:  95.16 | Prec:  93.14 | Rec:  97.27 || Ex/s: 119.54\n",
      "\n",
      "* Best F1: tensor(95.1590)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:  138.3 | Load Time:    1.4 || F1:  99.20 | Prec:  99.22 | Rec:  99.18 || Ex/s:  45.93\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:   17.4 | Load Time:    0.5 || F1:  94.63 | Prec:  92.29 | Rec:  97.08 || Ex/s: 119.69\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:  138.1 | Load Time:    1.4 || F1:  99.44 | Prec:  99.53 | Rec:  99.34 || Ex/s:  45.98\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:   17.3 | Load Time:    0.5 || F1:  94.27 | Prec:  90.47 | Rec:  98.40 || Ex/s: 120.17\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:  139.6 | Load Time:    1.4 || F1:  99.56 | Prec:  99.62 | Rec:  99.50 || Ex/s:  45.49\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:   17.4 | Load Time:    0.5 || F1:  94.50 | Prec:  91.97 | Rec:  97.17 || Ex/s: 119.54\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:  138.4 | Load Time:    1.4 || F1:  99.61 | Prec:  99.65 | Rec:  99.56 || Ex/s:  45.87\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:   17.3 | Load Time:    0.5 || F1:  94.34 | Prec:  93.60 | Rec:  95.10 || Ex/s: 119.76\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:  139.7 | Load Time:    1.4 || F1:  99.70 | Prec:  99.72 | Rec:  99.69 || Ex/s:  45.45\n",
      "\n",
      "===>  EVAL Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:   17.4 | Load Time:    0.5 || F1:  94.49 | Prec:  93.70 | Rec:  95.29 || Ex/s: 119.14\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:  138.5 | Load Time:    1.4 || F1:  99.73 | Prec:  99.78 | Rec:  99.69 || Ex/s:  45.84\n",
      "\n",
      "===>  EVAL Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:   17.5 | Load Time:    0.5 || F1:  94.49 | Prec:  93.62 | Rec:  95.38 || Ex/s: 118.87\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:  139.3 | Load Time:    1.4 || F1:  99.73 | Prec:  99.78 | Rec:  99.69 || Ex/s:  45.58\n",
      "\n",
      "===>  EVAL Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:   17.3 | Load Time:    0.5 || F1:  94.45 | Prec:  93.45 | Rec:  95.48 || Ex/s: 119.97\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:  140.4 | Load Time:    1.4 || F1:  99.73 | Prec:  99.78 | Rec:  99.69 || Ex/s:  45.25\n",
      "\n",
      "===>  EVAL Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:   17.6 | Load Time:    0.5 || F1:  94.36 | Prec:  93.28 | Rec:  95.48 || Ex/s: 117.92\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:  140.3 | Load Time:    1.4 || F1:  99.76 | Prec:  99.84 | Rec:  99.69 || Ex/s:  45.25\n",
      "\n",
      "===>  EVAL Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:   17.6 | Load Time:    0.5 || F1:  94.35 | Prec:  93.44 | Rec:  95.29 || Ex/s: 118.15\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   15.7 | Load Time:    0.5 || F1:  94.61 | Prec:  92.69 | Rec:  96.62 || Ex/s: 132.25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(94.6125)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sifDiff model.\n",
    "model_attention = dm.MatchingModel(attr_summarizer='attention')\n",
    "model_attention.initialize(train)  # Initilization\n",
    "\n",
    "# Train ing model on 15 epochs, batch size of 16, positive-to-negative \n",
    "# ratio to be 1:1. We save the best model (with the \n",
    "# highest F1 score on the validation set) to 'sifDiff_model.pth'.\n",
    "startTime = time.time()\n",
    "model_attention.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    best_save_path='attention_model.pth')\n",
    "executionTime_attention = (time.time() - startTime)/60\n",
    "\n",
    "# Evaluate the accuracy on the test data.\n",
    "model_attention.run_eval(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ww_1ydrmNe8E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ww_1ydrmNe8E",
    "outputId": "3837b8fa-f96d-4a2d-8fc5-cb38e458aa93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   15.6 | Load Time:    0.5 || F1:  94.61 | Prec:  92.69 | Rec:  96.62 || Ex/s: 132.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "model_attention = dm.MatchingModel(attr_summarizer='attention')\n",
    "model_attention.load_state('attention_model.pth')\n",
    "# Load the candidate set. Note that the trained model is an input parameter as we need to trained \n",
    "# model for candidate set preprocessing.\n",
    "predictions_attention = model_attention.run_prediction(test, output_attributes=list(test.get_raw_table().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uWtq5N4nNg7q",
   "metadata": {
    "id": "uWtq5N4nNg7q"
   },
   "outputs": [],
   "source": [
    "# get training data set and compute performance scores\n",
    "score_attention = scores(executionTime_attention, predictions_attention, 0, train_n.shape[0], model = 'attention')\n",
    "prec_matrix = prec_matrix.append(score_attention, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QpB4YfBXPalK",
   "metadata": {
    "id": "QpB4YfBXPalK"
   },
   "outputs": [],
   "source": [
    "high_score_pairs = predictions_attention[predictions_attention['match_score'] >= 0.9].sort_values(by=['match_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dUF_rghLPgkO",
   "metadata": {
    "id": "dUF_rghLPgkO"
   },
   "outputs": [],
   "source": [
    "predictions_attention['match_prediction'] = predictions_attention['match_score'].apply(lambda score: 1 if score >= 0.5 else 0)\n",
    "# Reorder columns to avoid scrolling...\n",
    "predictions_attention =  predictions_attention[['match_score', 'match_prediction'] + predictions_attention.columns.values[1:-1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xs3xYWLMlKyn",
   "metadata": {
    "id": "Xs3xYWLMlKyn"
   },
   "outputs": [],
   "source": [
    "path_predictions = os.path.join('.', 'Dissertation_Data_2', 'predictions_att.csv')\n",
    "predictions_attention.to_csv(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fjQTHe9tLLm7",
   "metadata": {
    "id": "fjQTHe9tLLm7"
   },
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Op8_nLU1K-0g",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op8_nLU1K-0g",
    "outputId": "2e32f7c3-4e5f-40f4-c5db-3105abe22ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 9210006\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:949: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:939: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:  296.0 | Load Time:    1.5 || F1:  90.50 | Prec:  89.73 | Rec:  91.28 || Ex/s:  21.57\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:   37.6 | Load Time:    0.5 || F1:  92.97 | Prec:  96.45 | Rec:  89.73 || Ex/s:  56.07\n",
      "\n",
      "* Best F1: tensor(92.9688)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:  295.8 | Load Time:    1.5 || F1:  95.86 | Prec:  95.77 | Rec:  95.95 || Ex/s:  21.59\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:   37.3 | Load Time:    0.5 || F1:  94.86 | Prec:  95.77 | Rec:  93.97 || Ex/s:  56.50\n",
      "\n",
      "* Best F1: tensor(94.8620)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:  294.9 | Load Time:    1.5 || F1:  97.06 | Prec:  96.96 | Rec:  97.15 || Ex/s:  21.65\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   37.1 | Load Time:    0.5 || F1:  95.59 | Prec:  94.15 | Rec:  97.08 || Ex/s:  56.82\n",
      "\n",
      "* Best F1: tensor(95.5917)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:  293.3 | Load Time:    1.5 || F1:  98.38 | Prec:  98.43 | Rec:  98.34 || Ex/s:  21.77\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:   37.3 | Load Time:    0.5 || F1:  94.59 | Prec:  91.39 | Rec:  98.02 || Ex/s:  56.46\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:  291.6 | Load Time:    1.5 || F1:  99.01 | Prec:  99.15 | Rec:  98.87 || Ex/s:  21.89\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:   37.0 | Load Time:    0.5 || F1:  94.22 | Prec:  90.39 | Rec:  98.40 || Ex/s:  57.03\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:  292.3 | Load Time:    1.5 || F1:  99.23 | Prec:  99.25 | Rec:  99.22 || Ex/s:  21.84\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   37.4 | Load Time:    0.5 || F1:  94.22 | Prec:  91.04 | Rec:  97.64 || Ex/s:  56.43\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:  292.6 | Load Time:    1.5 || F1:  99.53 | Prec:  99.62 | Rec:  99.44 || Ex/s:  21.82\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:   37.4 | Load Time:    0.5 || F1:  94.17 | Prec:  91.10 | Rec:  97.46 || Ex/s:  56.43\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:  292.3 | Load Time:    1.5 || F1:  99.65 | Prec:  99.75 | Rec:  99.56 || Ex/s:  21.84\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:   36.9 | Load Time:    0.5 || F1:  95.25 | Prec:  94.19 | Rec:  96.32 || Ex/s:  57.06\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:  293.2 | Load Time:    1.5 || F1:  99.78 | Prec:  99.91 | Rec:  99.66 || Ex/s:  21.77\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:   37.1 | Load Time:    0.5 || F1:  95.35 | Prec:  94.04 | Rec:  96.70 || Ex/s:  56.80\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:  292.7 | Load Time:    1.5 || F1:  99.78 | Prec:  99.91 | Rec:  99.66 || Ex/s:  21.81\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:   37.0 | Load Time:    0.5 || F1:  95.31 | Prec:  93.88 | Rec:  96.80 || Ex/s:  57.06\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:  290.9 | Load Time:    1.5 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  21.94\n",
      "\n",
      "===>  EVAL Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 11 || Run Time:   36.9 | Load Time:    0.5 || F1:  95.05 | Prec:  93.36 | Rec:  96.80 || Ex/s:  57.14\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:  291.2 | Load Time:    1.5 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  21.92\n",
      "\n",
      "===>  EVAL Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 12 || Run Time:   36.9 | Load Time:    0.5 || F1:  95.27 | Prec:  93.71 | Rec:  96.89 || Ex/s:  57.13\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:  292.4 | Load Time:    1.5 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  21.83\n",
      "\n",
      "===>  EVAL Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 13 || Run Time:   36.6 | Load Time:    0.5 || F1:  95.27 | Prec:  93.79 | Rec:  96.80 || Ex/s:  57.69\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:  289.8 | Load Time:    1.4 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  22.03\n",
      "\n",
      "===>  EVAL Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 14 || Run Time:   36.7 | Load Time:    0.5 || F1:  95.22 | Prec:  93.86 | Rec:  96.61 || Ex/s:  57.42\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:  292.6 | Load Time:    1.5 || F1:  99.87 | Prec:  99.94 | Rec:  99.81 || Ex/s:  21.82\n",
      "\n",
      "===>  EVAL Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 15 || Run Time:   37.1 | Load Time:    0.5 || F1:  95.22 | Prec:  93.86 | Rec:  96.61 || Ex/s:  56.78\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   32.6 | Load Time:    0.5 || F1:  95.30 | Prec:  94.67 | Rec:  95.95 || Ex/s:  64.65\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(95.3020)"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sifDiff model.\n",
    "model_hyb = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "model_hyb.initialize(train)  # Initilization\n",
    "\n",
    "# Train ing model on 15 epochs, batch size of 16, positive-to-negative \n",
    "# ratio to be 1:1. We save the best model (with the \n",
    "# highest F1 score on the validation set) to 'sifDiff_model.pth'.\n",
    "startTime = time.time()\n",
    "model_hyb.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    best_save_path='hybrid_model.pth')\n",
    "executionTime_hyb = (time.time() - startTime)/60\n",
    "\n",
    "# Evaluate the accuracy on the test data.\n",
    "model_hyb.run_eval(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nhYg9HNuEU_T",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nhYg9HNuEU_T",
    "outputId": "69df5adc-8620-4959-8fb9-090bd5023620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   32.3 | Load Time:    0.5 || F1:  95.30 | Prec:  94.67 | Rec:  95.95 || Ex/s:  65.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "model_hyb = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "model_hyb.load_state('hybrid_model.pth')\n",
    "# Load the candidate set. Note that the trained model is an input parameter as we need to trained \n",
    "# model for candidate set preprocessing.\n",
    "predictions_hyb = model_hyb.run_prediction(test, output_attributes=list(test.get_raw_table().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wDqUCuiQErWZ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wDqUCuiQErWZ"
   },
   "outputs": [],
   "source": [
    "# get training data set and compute performance scores\n",
    "score_hyb = scores(executionTime_hyb, predictions_hyb, 0, train_n.shape[0], model = 'hybrid')\n",
    "prec_matrix =prec_matrix.append(score_hyb, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JQHj76QgEaPp",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JQHj76QgEaPp"
   },
   "outputs": [],
   "source": [
    "high_score_pairs = predictions_hyb[predictions_hyb['match_score'] >= 0.9].sort_values(by=['match_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Na9iNHh0EeP4",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Na9iNHh0EeP4"
   },
   "outputs": [],
   "source": [
    "predictions_hyb['match_prediction'] = predictions_hyb['match_score'].apply(lambda score: 1 if score >= 0.5 else 0)\n",
    "# Reorder columns to avoid scrolling...\n",
    "predictions_hyb =  predictions_hyb[['match_score', 'match_prediction'] + predictions_hyb.columns.values[1:-1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F7f-CEWhGcH0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "F7f-CEWhGcH0"
   },
   "outputs": [],
   "source": [
    "path_DL_res = os.path.join('.', 'Dissertation_Data_2', 'DL_res.csv')\n",
    "prec_matrix.to_csv(path_DL_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JxWWnjYCk-Nk",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JxWWnjYCk-Nk"
   },
   "outputs": [],
   "source": [
    "path_predictions = os.path.join('.', 'Dissertation_Data_2', 'predictions_hyb.csv')\n",
    "predictions_hyb.to_csv(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ycaxMgk7nZYB",
   "metadata": {
    "id": "ycaxMgk7nZYB"
   },
   "source": [
    "## ML Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZDNMr74Y9Z9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KZDNMr74Y9Z9",
    "outputId": "e5fcfa58-dedf-4003-b193-063faca56971"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py_entitymatching.io.parsers:Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _id     ltable_id  ... rtable_year label\n",
      "0    0  019zSr3Lx4EJ  ...        1995     1\n",
      "1    1  KureLsKbUXYJ  ...        1995     1\n",
      "2    2  T2fm7Wb1ak4J  ...        1995     1\n",
      "3    3  iWNLOYCQX-YJ  ...        1995     1\n",
      "4    4  wgK6p4mDSIMJ  ...        2002     1\n",
      "\n",
      "[5 rows x 12 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/py_entitymatching/matcher/matcherutils.py:224: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  imp.statistics_[pd.np.isnan(imp.statistics_)] = val_all_nans\n"
     ]
    }
   ],
   "source": [
    "# creating dataset for ML matching\n",
    "S = em.read_csv_metadata(path_cand, \n",
    "                         key='_id',\n",
    "                         ltable=Scholar, rtable=DBLP, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "print(S.head())\n",
    "# Split S into training and test data\n",
    "IJ = em.split_train_test(S, train_proportion=0.6, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']\n",
    "\n",
    "# Generate a set of features\n",
    "F = em.get_features_for_matching(Scholar, DBLP, validate_inferred_attr_types=False)\n",
    "\n",
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=F, \n",
    "                            attrs_after='label',\n",
    "                            show_progress=False)\n",
    "\n",
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "\n",
    "# Impute feature vectors with the mean of the column values.\n",
    "H = em.impute_table(H, \n",
    "                exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'],\n",
    "                missing_val = np.nan,\n",
    "                strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcI5sKqDsIxK",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mcI5sKqDsIxK",
    "outputId": "f3b80bb5-e23f-4f6f-d5fc-0f9937683b8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Average recall</th>\n",
       "      <th>Average f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.874217</td>\n",
       "      <td>0.852714</td>\n",
       "      <td>0.863236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.916346</td>\n",
       "      <td>0.897656</td>\n",
       "      <td>0.906668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.853588</td>\n",
       "      <td>0.746811</td>\n",
       "      <td>0.795985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>0.835670</td>\n",
       "      <td>0.837247</td>\n",
       "      <td>0.836193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.832198</td>\n",
       "      <td>0.832182</td>\n",
       "      <td>0.831876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Matcher  Average precision  Average recall  Average f1\n",
       "0  DecisionTree           0.874217        0.852714    0.863236\n",
       "1            RF           0.916346        0.897656    0.906668\n",
       "2           SVM           0.853588        0.746811    0.795985\n",
       "3        LinReg           0.835670        0.837247    0.836193\n",
       "4        LogReg           0.832198        0.832182    0.831876"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'],\n",
    "        k=10,\n",
    "        target_attr='label', metric_to_select_matcher='f1', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dXQAJMEjsMWu",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dXQAJMEjsMWu",
    "outputId": "dbe766ee-63e8-4b3c-8e4e-4075eda35c71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Num folds</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Fold 6</th>\n",
       "      <th>Fold 7</th>\n",
       "      <th>Fold 8</th>\n",
       "      <th>Fold 9</th>\n",
       "      <th>Fold 10</th>\n",
       "      <th>Mean score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>&lt;py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x7fe083e33790&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.857595</td>\n",
       "      <td>0.899687</td>\n",
       "      <td>0.874608</td>\n",
       "      <td>0.875399</td>\n",
       "      <td>0.854890</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.873288</td>\n",
       "      <td>0.866221</td>\n",
       "      <td>0.874217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x7fe083e1a710&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.899054</td>\n",
       "      <td>0.926380</td>\n",
       "      <td>0.943144</td>\n",
       "      <td>0.928339</td>\n",
       "      <td>0.899054</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900332</td>\n",
       "      <td>0.909396</td>\n",
       "      <td>0.916346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x7fe083525490&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>0.875887</td>\n",
       "      <td>0.861004</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.844203</td>\n",
       "      <td>0.867159</td>\n",
       "      <td>0.878327</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.814035</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.853588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x7fe083e1ab90&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827795</td>\n",
       "      <td>0.843844</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.834983</td>\n",
       "      <td>0.831210</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.840391</td>\n",
       "      <td>0.835670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x7fe083e1a5d0&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.821958</td>\n",
       "      <td>0.846626</td>\n",
       "      <td>0.845161</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.826797</td>\n",
       "      <td>0.829582</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.824841</td>\n",
       "      <td>0.832198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  ... Mean score\n",
       "0  DecisionTree  ...   0.874217\n",
       "1            RF  ...   0.916346\n",
       "2           SVM  ...   0.853588\n",
       "3        LinReg  ...   0.835670\n",
       "4        LogReg  ...   0.832198\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['drill_down_cv_stats']['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YH7AKEMLRYR",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_YH7AKEMLRYR"
   },
   "outputs": [],
   "source": [
    "path_ML_res = os.path.join('.', 'Dissertation_Data_2', 'ML_res.csv')\n",
    "\n",
    "result['cv_stats'].to_csv(path_ML_res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Dissertation_SchDBLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
